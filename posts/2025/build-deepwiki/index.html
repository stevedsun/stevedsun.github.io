<!doctype html><html lang=zh-CN><head><meta http-equiv=X-Clacks-Overhead content="GNU Terry Pratchett"><meta charset=utf-8><meta name=viewport content="width=device-width,initial-scale=1"><link rel="shortcut icon" href=https://sund.site/favicon.ico><title>DeepWIKI 是如何工作的 | Steve Sun</title>
<meta name=title content="DeepWIKI 是如何工作的"><meta name=description content="DeepWIKI 是一个从源代码仓库生成详细文档的 AI Agent 项目，由 Devin.ai 提供。自从它火了以后，我就一直非常好奇它是怎么工作的。
我梳理了网上的相关资料和一些开源项目，得到了相对清晰的工作流程。对于其中难点的部分，我会在后续文章中跟进我的发现。
生成代码结构地图
首先 DeepWIKI 本质是一个 RAG 系统，它读取源代码仓库作为输入，将代码进行语法分析之后转换成代表语法结构和文件结构的元数据和代表代码描述和片段的向量数据两部分，元数据存到关系数据库中，同时将对应的代码片段存储到向量数据库中以便后续 LLM 检索。
生成 WIKI 页面
生成 WIKI 页面的过程，就是 RAG 系统 query 的过程：

程序递归读取项目结构。
从元数据库中查询当前文件的元数据，再从向量数据库中查找相关性最强的代码和描述信息的 id。
用这些 id 再去元数据库里查询到描述信息，从工程文件中查询对应代码片段。
将上面的所有内容作为 context，根据元数据类型（架构、组件等）组合适当的 prompt，输入给 LLM。
最后由一个前端渲染引擎把 LLM 的输出渲染成文档页面。
重复步骤 1。


难点 1：分块策略
上述过程中，如何在嵌入（embedding）前给代码分块，是个比较值得研究的话题。一般自然语言的分块是基于段落、句子、标点符号等方式，拆分出来的 chunk 包含完整的句子或者段落上下文。
但是代码的拆分不同，比如一个函数体由{ }包裹起来，如果使用自然语言的分词器分词，会导致上下文被拆分到不同 chunk 中，后续检索向量时准确度就会下降。
目前的解决办法有两种，一种是基于整个文件的分块，这种情况文件大小不能超过分块大小的上限，而且分块数据缺少真实的调用关系上下文。我们知道，代码的组织单元并不是文件（文件树只是方便人类阅读的组织形式），而是以类和函数为单元的网状依赖关系图。
第二种方式就是先用语法工具对代码文件做静态分析，再根据分析结果将代码以语法结构进行拆分。这种方式实现复杂，网上并没有找到相关的资料，幸而读到这篇RAG for a Codebase with 10k Repos，它介绍了如何利用语法静态分析来给代码分块，构建高效的代码仓库 RAG 系统。 但是文章也没有提供开源实现，考虑到作为商业项目的核心技术，这部分内容非常值得深入。我会持续跟进这部分内容的研究。
难点 2: 解析语法结构
元数据的语法解析要比向量数据简单一些，我从另一个开源项目Repo Graph中找到一些线索。
这个项目使用了 tree-sitter 来分析项目语法结构，从而得到三类元数据文件："><meta name=keywords content="AI,"><link rel=canonical href=https://sund.site/posts/2025/build-deepwiki/><meta property="og:url" content="https://sund.site/posts/2025/build-deepwiki/"><meta property="og:site_name" content="Steve Sun"><meta property="og:title" content="DeepWIKI 是如何工作的"><meta property="og:description" content="DeepWIKI 是一个从源代码仓库生成详细文档的 AI Agent 项目，由 Devin.ai 提供。自从它火了以后，我就一直非常好奇它是怎么工作的。
我梳理了网上的相关资料和一些开源项目，得到了相对清晰的工作流程。对于其中难点的部分，我会在后续文章中跟进我的发现。
生成代码结构地图 首先 DeepWIKI 本质是一个 RAG 系统，它读取源代码仓库作为输入，将代码进行语法分析之后转换成代表语法结构和文件结构的元数据和代表代码描述和片段的向量数据两部分，元数据存到关系数据库中，同时将对应的代码片段存储到向量数据库中以便后续 LLM 检索。
生成 WIKI 页面 生成 WIKI 页面的过程，就是 RAG 系统 query 的过程：
程序递归读取项目结构。 从元数据库中查询当前文件的元数据，再从向量数据库中查找相关性最强的代码和描述信息的 id。 用这些 id 再去元数据库里查询到描述信息，从工程文件中查询对应代码片段。 将上面的所有内容作为 context，根据元数据类型（架构、组件等）组合适当的 prompt，输入给 LLM。 最后由一个前端渲染引擎把 LLM 的输出渲染成文档页面。 重复步骤 1。 难点 1：分块策略 上述过程中，如何在嵌入（embedding）前给代码分块，是个比较值得研究的话题。一般自然语言的分块是基于段落、句子、标点符号等方式，拆分出来的 chunk 包含完整的句子或者段落上下文。
但是代码的拆分不同，比如一个函数体由{ }包裹起来，如果使用自然语言的分词器分词，会导致上下文被拆分到不同 chunk 中，后续检索向量时准确度就会下降。
目前的解决办法有两种，一种是基于整个文件的分块，这种情况文件大小不能超过分块大小的上限，而且分块数据缺少真实的调用关系上下文。我们知道，代码的组织单元并不是文件（文件树只是方便人类阅读的组织形式），而是以类和函数为单元的网状依赖关系图。
第二种方式就是先用语法工具对代码文件做静态分析，再根据分析结果将代码以语法结构进行拆分。这种方式实现复杂，网上并没有找到相关的资料，幸而读到这篇RAG for a Codebase with 10k Repos，它介绍了如何利用语法静态分析来给代码分块，构建高效的代码仓库 RAG 系统。 但是文章也没有提供开源实现，考虑到作为商业项目的核心技术，这部分内容非常值得深入。我会持续跟进这部分内容的研究。
难点 2: 解析语法结构 元数据的语法解析要比向量数据简单一些，我从另一个开源项目Repo Graph中找到一些线索。
这个项目使用了 tree-sitter 来分析项目语法结构，从而得到三类元数据文件："><meta property="og:locale" content="zh_CN"><meta property="og:type" content="article"><meta property="article:section" content="posts"><meta property="article:published_time" content="2025-05-24T12:50:40+08:00"><meta property="article:modified_time" content="2025-05-24T12:50:40+08:00"><meta property="article:tag" content="AI"><meta property="og:image" content="https://sund.site/images/share.png"><meta name=twitter:card content="summary_large_image"><meta name=twitter:image content="https://sund.site/images/share.png"><meta name=twitter:title content="DeepWIKI 是如何工作的"><meta name=twitter:description content="DeepWIKI 是一个从源代码仓库生成详细文档的 AI Agent 项目，由 Devin.ai 提供。自从它火了以后，我就一直非常好奇它是怎么工作的。
我梳理了网上的相关资料和一些开源项目，得到了相对清晰的工作流程。对于其中难点的部分，我会在后续文章中跟进我的发现。
生成代码结构地图 首先 DeepWIKI 本质是一个 RAG 系统，它读取源代码仓库作为输入，将代码进行语法分析之后转换成代表语法结构和文件结构的元数据和代表代码描述和片段的向量数据两部分，元数据存到关系数据库中，同时将对应的代码片段存储到向量数据库中以便后续 LLM 检索。
生成 WIKI 页面 生成 WIKI 页面的过程，就是 RAG 系统 query 的过程：
程序递归读取项目结构。 从元数据库中查询当前文件的元数据，再从向量数据库中查找相关性最强的代码和描述信息的 id。 用这些 id 再去元数据库里查询到描述信息，从工程文件中查询对应代码片段。 将上面的所有内容作为 context，根据元数据类型（架构、组件等）组合适当的 prompt，输入给 LLM。 最后由一个前端渲染引擎把 LLM 的输出渲染成文档页面。 重复步骤 1。 难点 1：分块策略 上述过程中，如何在嵌入（embedding）前给代码分块，是个比较值得研究的话题。一般自然语言的分块是基于段落、句子、标点符号等方式，拆分出来的 chunk 包含完整的句子或者段落上下文。
但是代码的拆分不同，比如一个函数体由{ }包裹起来，如果使用自然语言的分词器分词，会导致上下文被拆分到不同 chunk 中，后续检索向量时准确度就会下降。
目前的解决办法有两种，一种是基于整个文件的分块，这种情况文件大小不能超过分块大小的上限，而且分块数据缺少真实的调用关系上下文。我们知道，代码的组织单元并不是文件（文件树只是方便人类阅读的组织形式），而是以类和函数为单元的网状依赖关系图。
第二种方式就是先用语法工具对代码文件做静态分析，再根据分析结果将代码以语法结构进行拆分。这种方式实现复杂，网上并没有找到相关的资料，幸而读到这篇RAG for a Codebase with 10k Repos，它介绍了如何利用语法静态分析来给代码分块，构建高效的代码仓库 RAG 系统。 但是文章也没有提供开源实现，考虑到作为商业项目的核心技术，这部分内容非常值得深入。我会持续跟进这部分内容的研究。
难点 2: 解析语法结构 元数据的语法解析要比向量数据简单一些，我从另一个开源项目Repo Graph中找到一些线索。
这个项目使用了 tree-sitter 来分析项目语法结构，从而得到三类元数据文件："><meta itemprop=name content="DeepWIKI 是如何工作的"><meta itemprop=description content="DeepWIKI 是一个从源代码仓库生成详细文档的 AI Agent 项目，由 Devin.ai 提供。自从它火了以后，我就一直非常好奇它是怎么工作的。
我梳理了网上的相关资料和一些开源项目，得到了相对清晰的工作流程。对于其中难点的部分，我会在后续文章中跟进我的发现。
生成代码结构地图 首先 DeepWIKI 本质是一个 RAG 系统，它读取源代码仓库作为输入，将代码进行语法分析之后转换成代表语法结构和文件结构的元数据和代表代码描述和片段的向量数据两部分，元数据存到关系数据库中，同时将对应的代码片段存储到向量数据库中以便后续 LLM 检索。
生成 WIKI 页面 生成 WIKI 页面的过程，就是 RAG 系统 query 的过程：
程序递归读取项目结构。 从元数据库中查询当前文件的元数据，再从向量数据库中查找相关性最强的代码和描述信息的 id。 用这些 id 再去元数据库里查询到描述信息，从工程文件中查询对应代码片段。 将上面的所有内容作为 context，根据元数据类型（架构、组件等）组合适当的 prompt，输入给 LLM。 最后由一个前端渲染引擎把 LLM 的输出渲染成文档页面。 重复步骤 1。 难点 1：分块策略 上述过程中，如何在嵌入（embedding）前给代码分块，是个比较值得研究的话题。一般自然语言的分块是基于段落、句子、标点符号等方式，拆分出来的 chunk 包含完整的句子或者段落上下文。
但是代码的拆分不同，比如一个函数体由{ }包裹起来，如果使用自然语言的分词器分词，会导致上下文被拆分到不同 chunk 中，后续检索向量时准确度就会下降。
目前的解决办法有两种，一种是基于整个文件的分块，这种情况文件大小不能超过分块大小的上限，而且分块数据缺少真实的调用关系上下文。我们知道，代码的组织单元并不是文件（文件树只是方便人类阅读的组织形式），而是以类和函数为单元的网状依赖关系图。
第二种方式就是先用语法工具对代码文件做静态分析，再根据分析结果将代码以语法结构进行拆分。这种方式实现复杂，网上并没有找到相关的资料，幸而读到这篇RAG for a Codebase with 10k Repos，它介绍了如何利用语法静态分析来给代码分块，构建高效的代码仓库 RAG 系统。 但是文章也没有提供开源实现，考虑到作为商业项目的核心技术，这部分内容非常值得深入。我会持续跟进这部分内容的研究。
难点 2: 解析语法结构 元数据的语法解析要比向量数据简单一些，我从另一个开源项目Repo Graph中找到一些线索。
这个项目使用了 tree-sitter 来分析项目语法结构，从而得到三类元数据文件："><meta itemprop=datePublished content="2025-05-24T12:50:40+08:00"><meta itemprop=dateModified content="2025-05-24T12:50:40+08:00"><meta itemprop=wordCount content="243"><meta itemprop=image content="https://sund.site/images/share.png"><meta itemprop=keywords content="AI"><meta name=referrer content="no-referrer-when-downgrade"><style>:root{--width-max:720px;--font-primary:Verdana, sans-serif;--font-secondary:monospace;--font-size-primary:1em;--font-size-secondary:0.8em;--body-bg-color:#fcfcfc;--bold-text-color:#222;--body-text-color:#444;--link-color:#3273dc;--link-visited-color:#8b6fcb;--table-border-color:#f2f2f2;--table-th-bg-color:#f2f2f2;--img-border-color:#f2f2f2;--code-bg-color:#f2f2f2;--code-text-color:#222;--blockquote-border-color:#666;--blockquote-text-color:#666;--upvoted-color:#FA8072}@media(prefers-color-scheme:dark){:root{--body-bg-color:#1c1c1c;--bold-text-color:#eee;--body-text-color:#ddd;--link-color:#8cc2dd;--link-visited-color:#c3b1ee;--table-border-color:#999;--table-th-bg-color:#999;--img-border-color:#999;--code-bg-color:#555;--code-text-color:#ddd;--blockquote-border-color:#ccc;--blockquote-text-color:#ccc}}body{font-family:var(--font-primary);font-size:var(--font-size-primary);margin:auto;padding:20px;max-width:var(--width-max);text-align:left;background-color:var(--body-bg-color);word-wrap:break-word;overflow-wrap:break-word;line-height:1.5;color:var(--body-text-color)}h1,h2,h3,h4,h5,h6,strong,b{color:var(--bold-text-color)}h1,h2,h3,h4,h5,h6{margin:16px 0}a{color:var(--link-color);cursor:pointer;text-decoration:none}a:hover{text-decoration:underline}.title{text-decoration:none;border:0}.title:hover{text-decoration:none}.title span{font-weight:400}nav a{margin-right:8px}textarea{width:100%;font-size:16px}input{font-size:14px}content{line-height:1.6}table{width:100%;border-collapse:collapse;border:1px solid var(--table-border-color);border-radius:4px}th,td{border:1px solid var(--table-border-color);padding:4px}th{background-color:var(--table-th-bg-color)}hr{border:0;border-top:1px dashed}img{max-width:100%;display:block;margin-left:auto;margin-right:auto;border:1px solid var(--img-border-color);border-radius:4px;content-visibility:auto;loading:lazy}img[src*="#minipic"]{max-width:50%;margin-left:0;margin-right:auto}i{font-style:normal}time{font-family:var(--font-secondary);font-size:15px}code{font-family:var(--font-secondary);background-color:var(--code-bg-color);color:var(--code-text-color);padding:2px;border-radius:4px}pre code{display:block;padding:16px;white-space:pre-wrap;overflow-x:auto}div.highlight pre{border-radius:4px}div.highlight code{background-color:var(--code-bg-color);color:var(--code-text-color)}blockquote{border-left:2px solid var(--blockquote-border-color);color:var(--blockquote-text-color);margin:0;padding-left:16px;font-style:normal}blockquote p{margin:0}footer{padding:25px 0;text-align:center;font-size:var(--font-size-secondary)}ul li:has(input){list-style-type:none;margin-left:-25.5px}ul.blog-posts{list-style-type:none;padding:unset}ul.blog-posts li{display:flex}ul.blog-posts li span{flex:0 0 130px}ul.blog-posts li span.grouped{flex:0 0 80px}ul.blog-posts li a:visited{color:var(--link-visited-color)}div.toc{position:fixed;top:50%;left:calc((100vw + var(--width-max))/2);transform:translateY(-50%);width:calc((90vw - var(--width-max))/2);max-height:80vh;overflow-y:auto;padding:20px 8px;z-index:99;&::-webkit-scrollbar { display:none; } -ms-overflow-style:none;scrollbar-width:none}div.toc ul{list-style-type:none;padding-left:0}div.toc ul li{margin:8px 0}div.toc ul li a{text-decoration:none;color:var(--blockquote-text-color)}div.toc ul li a:hover{color:var(--link-color)}button.upvote-btn{margin:0;padding:0;border:none;background:0 0;cursor:pointer;display:flex;flex-direction:column;align-items:center;color:var(--body-text-color)}button.upvoted{color:var(--upvoted-color)}span.upvote-count{margin-top:-4px;font-size:smaller}@media(max-width:500px){img[src*="#minipic"]{max-width:100%;margin-left:auto;margin-right:auto}div.toc{display:none}}</style><link rel=stylesheet href=https://chinese-fonts-cdn.deno.dev/packages/lxgwwenkai/dist/LXGWWenKai-Regular/result.css><style>body,div.post-body,h1,h2,h3,h4{font-family:lxgw wenkai,sans-serif;font-size:18px}</style><script async src="https://www.googletagmanager.com/gtag/js?id=G-XJJVVQ0LBH"></script><script>window.dataLayer=window.dataLayer||[];function gtag(){dataLayer.push(arguments)}gtag("js",new Date),gtag("config","G-XJJVVQ0LBH")</script></head><body><header><a href=/ class=title><h1>Steve Sun</h1></a><nav><a href=/>Home</a>
<a href=/friends/>Friends</a>
<a href=/posts/>Posts</a></nav></header><main><h1>DeepWIKI 是如何工作的</h1><p><i><time datetime=2025-05-24 pubdate>24 May, 2025</time></i></p><content><p><a href=https://deepwiki.com>DeepWIKI</a> 是一个从源代码仓库生成详细文档的 AI Agent 项目，由 Devin.ai 提供。自从它火了以后，我就一直非常好奇它是怎么工作的。</p><p>我梳理了网上的相关资料和一些开源项目，得到了相对清晰的工作流程。对于其中难点的部分，我会在后续文章中跟进我的发现。</p><h2 id=生成代码结构地图>生成代码结构地图</h2><p>首先 DeepWIKI 本质是一个 RAG 系统，它读取源代码仓库作为输入，将代码进行语法分析之后转换成<strong>代表语法结构和文件结构的元数据</strong>和<strong>代表代码描述和片段的向量数据</strong>两部分，元数据存到关系数据库中，同时将对应的代码片段存储到向量数据库中以便后续 LLM 检索。</p><h2 id=生成-wiki-页面>生成 WIKI 页面</h2><p>生成 WIKI 页面的过程，就是 RAG 系统 query 的过程：</p><ol><li>程序递归读取项目结构。</li><li>从元数据库中查询当前文件的元数据，再从向量数据库中查找相关性最强的代码和描述信息的 id。</li><li>用这些 id 再去元数据库里查询到描述信息，从工程文件中查询对应代码片段。</li><li>将上面的所有内容作为 context，根据元数据类型（架构、组件等）组合适当的 prompt，输入给 LLM。</li><li>最后由一个前端渲染引擎把 LLM 的输出渲染成文档页面。</li><li>重复步骤 1。</li></ol><p><img src=https://www.gptsecurity.info/img/in-post/rag_flow.png alt=图片来自https://www.gptsecurity.info/2024/05/26/RAG/></p><h2 id=难点-1分块策略>难点 1：分块策略</h2><p>上述过程中，如何在嵌入（embedding）前给代码分块，是个比较值得研究的话题。一般自然语言的分块是基于段落、句子、标点符号等方式，拆分出来的 chunk 包含完整的句子或者段落上下文。</p><p>但是代码的拆分不同，比如一个函数体由<code>{</code> <code>}</code>包裹起来，如果使用自然语言的分词器分词，会导致上下文被拆分到不同 chunk 中，后续检索向量时准确度就会下降。</p><p>目前的解决办法有两种，一种是基于整个文件的分块，这种情况文件大小不能超过分块大小的上限，而且分块数据缺少真实的调用关系上下文。我们知道，代码的组织单元并不是文件（文件树只是方便人类阅读的组织形式），而是以类和函数为单元的网状依赖关系图。</p><p>第二种方式就是先用语法工具对代码文件做静态分析，再根据分析结果将代码以语法结构进行拆分。这种方式实现复杂，网上并没有找到相关的资料，幸而读到这篇<a href=https://www.qodo.ai/blog/rag-for-large-scale-code-repos/>RAG for a Codebase with 10k Repos</a>，它介绍了如何利用语法静态分析来给代码分块，构建高效的代码仓库 RAG 系统。 但是文章也没有提供开源实现，考虑到作为商业项目的核心技术，这部分内容非常值得深入。我会持续跟进这部分内容的研究。</p><h2 id=难点-2-解析语法结构>难点 2: 解析语法结构</h2><p>元数据的语法解析要比向量数据简单一些，我从另一个开源项目<a href=https://github.com/ozyyshr/RepoGraph>Repo Graph</a>中找到一些线索。</p><p>这个项目使用了 <code>tree-sitter</code> 来分析项目语法结构，从而得到三类元数据文件：</p><ul><li><code>tag.json</code>：代表一个文件、函数、类的路径、行号、描述等基础信息。</li><li><code>tree_structure.json</code>: 项目的文件树结构信息。</li><li><code>*.pkl</code>: 对象依赖关系图。</li></ul><p><code>*.pkl</code>是语法分析器扫描项目文件之后得到的一个网状的对象关系图，它使用 python 的 pickle 库把 python 网状对象序列化成文件。</p><p>从这个项目的实现来看，难点 1 中嵌入向量的过程似乎也可以用 <code>tree-sitter</code> 生成的代码元信息对代码按行分块。</p><h2 id=提示词工程>提示词工程</h2><p>在 RAG 查询阶段，要根据当前元信息的类型，组装不同的提示词。</p><p>这个项目<a href=https://github.com/metauto-ai/agent-as-a-judge>Agent as a Judge</a> 里有不少提示词可供参考：</p><p>生成概述的提示词</p><pre tabindex=0><code>Provide a concise overview of this repository focused primarily on:
* Purpose and Scope: What is this project&#39;s main purpose?
* Core Features: What are the key features and capabilities?
* Target audience/users
* Main technologies or frameworks used
</code></pre><p>生成架构文档的提示词</p><pre tabindex=0><code>Create a comprehensive architecture overview for this repository. Include:
* A high-level description of the system architecture
* Main components and their roles
* Data flow between components
* External dependencies and integrations
</code></pre><p>生成组件文档的提示词</p><pre tabindex=0><code>Provide a comprehensive analysis of all key components in this codebase. For each component:
* Name of the component
* Purpose and main responsibility
* How it interacts with other components
* Design patterns or techniques used
* Key characteristics
* File paths that implement this component
</code></pre><p>其余请参考项目文件，就不一一列举了。</p><h2 id=总结>总结</h2><p>DeepWIKI 是一个基于 RAG 系统的代码文档生成工具，它通过以下步骤工作：</p><ol><li>对代码仓库进行语法分析，生成元数据和向量数据</li><li>然后通过 RAG 系统查询这些数据来生成文档</li><li>最后用前端引擎渲染成可读的文档页面</li></ol><p>实现过程中有两个主要难点：</p><ul><li>代码分块策略：需要考虑代码的语法结构，不能像自然语言那样简单分割</li><li>语法结构解析：可以使用 tree-sitter 等工具来解析代码结构</li></ul><p>虽然目前有一些开源项目可以参考，但核心的分块策略实现仍然需要深入研究。</p><h2 id=参考项目>参考项目</h2><ul><li><a href=https://github.com/metauto-ai/agent-as-a-judge>Agent as a Judge</a></li><li><a href=https://github.com/ozyyshr/RepoGraph>Repo Graph</a></li><li><a href=https://github.com/AsyncFuncAI/deepwiki-open>DeepWiki Open</a></li></ul></content><p><a href=https://sund.site/tags/ai/>#AI</a></p><div class=toc><nav id=TableOfContents><ul><li><a href=#生成代码结构地图>生成代码结构地图</a></li><li><a href=#生成-wiki-页面>生成 WIKI 页面</a></li><li><a href=#难点-1分块策略>难点 1：分块策略</a></li><li><a href=#难点-2-解析语法结构>难点 2: 解析语法结构</a></li><li><a href=#提示词工程>提示词工程</a></li><li><a href=#总结>总结</a></li><li><a href=#参考项目>参考项目</a></li></ul></nav></div></main><footer>Subscribe via <a href=/index.xml>RSS</a>.<br>Made with
<a href=https://github.com/rokcso/hugo-bearblog-neo/>Hugo Bear Neo</a>.<br>Copyright © 2013-2025, Steve Sun.
🗺️ <a href=/sitemap.xml>Sitemap</a>.</footer></body></html>