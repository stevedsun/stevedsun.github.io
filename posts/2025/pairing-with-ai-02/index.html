<!doctype html><html lang=zh-CN><head><meta http-equiv=X-Clacks-Overhead content="GNU Terry Pratchett"><meta charset=utf-8><meta name=viewport content="width=device-width,initial-scale=1"><link rel="shortcut icon" href=https://sund.site/favicon.ico><title>与AI协作编程──痛点篇 | Steve Sun</title>
<meta name=title content="与AI协作编程──痛点篇"><meta name=description content="在与 AI 协作编程中，经常遇到一些大模型无法正确执行的情况。最常见的有：

任务死循环
模型无法修复环境问题
模型执行长任务后半段忘记上下文

一些使用经验
以我自己为例，我经常使用 Cline + Github Copilot 的组合。我很喜欢 Cline 的功能是 Checkpoint restore，它可以在执行错误的位置重新编辑提示词执行。这让我可以在相同的任务中调用不同的模型，观察他们处理问题的能力。
用作规划（Plan）的模型通常用 Deepseek-R1，Gemini 2.0 Flash Thinking，Claude 3.7。这里除了 Claude 3.7 能够比较准确给出计划外，其他模型多少都容易走「歪路」， 比如 Deepseek-R1 喜欢做一些多余的事情，让它翻译中文，它会调用 MCP 的翻译服务而不是自己翻译。
从经济角度考虑，解决简单问题 Gemini 2.0 Flash Thinking 是比较快速、经济的模型。复杂问题直接上 Claude 3.7 可能更容易控制成本。
用作执行任务（Act）的模型里，Deepseek-V3 表现非常不稳定，经常死循环或丢失上下文。Claude 太贵，而 Gemini 2.0 Flash 是相对准确且划算的模型。置于国产的 Qwen 系列模型不完全支持 Function Calling，Cline 也没有适配，所以暂时无法测试。
AI 编程疑难杂症的应对方法
最近读到AI Blindspots这篇文章，作者系统性整理了 AI 编程中遇到的问题和他的思路。对我非常有启发。我用 Agent 把它翻译成了中文并人工做了润色，你可以在这里读到：AI 编程的盲点。
概括起来，解决 AI 问题的核心要领还是三点：更准确的提示词、更完整的上下文、缩小问题规模。
相信随着技术的发展，编程范式会发生翻天覆地的变化。如果重构变得如此容易，那么马丁福勒的《重构》是否应该出一套 AI 时代下的新范式。如果文档不再是被人读，而是喂给模型当作上下文，那么文档的形态应该是什么样？是否提供一个向量化的文档接口供大模型调用，将是未来编程框架的新常态？
我对未来充满期待。"><meta name=keywords content="AI,"><link rel=canonical href=https://sund.site/posts/2025/pairing-with-ai-02/><meta property="og:url" content="https://sund.site/posts/2025/pairing-with-ai-02/"><meta property="og:site_name" content="Steve Sun"><meta property="og:title" content="与AI协作编程──痛点篇"><meta property="og:description" content="在与 AI 协作编程中，经常遇到一些大模型无法正确执行的情况。最常见的有：
任务死循环 模型无法修复环境问题 模型执行长任务后半段忘记上下文 一些使用经验 以我自己为例，我经常使用 Cline + Github Copilot 的组合。我很喜欢 Cline 的功能是 Checkpoint restore，它可以在执行错误的位置重新编辑提示词执行。这让我可以在相同的任务中调用不同的模型，观察他们处理问题的能力。
用作规划（Plan）的模型通常用 Deepseek-R1，Gemini 2.0 Flash Thinking，Claude 3.7。这里除了 Claude 3.7 能够比较准确给出计划外，其他模型多少都容易走「歪路」， 比如 Deepseek-R1 喜欢做一些多余的事情，让它翻译中文，它会调用 MCP 的翻译服务而不是自己翻译。
从经济角度考虑，解决简单问题 Gemini 2.0 Flash Thinking 是比较快速、经济的模型。复杂问题直接上 Claude 3.7 可能更容易控制成本。
用作执行任务（Act）的模型里，Deepseek-V3 表现非常不稳定，经常死循环或丢失上下文。Claude 太贵，而 Gemini 2.0 Flash 是相对准确且划算的模型。置于国产的 Qwen 系列模型不完全支持 Function Calling，Cline 也没有适配，所以暂时无法测试。
AI 编程疑难杂症的应对方法 最近读到AI Blindspots这篇文章，作者系统性整理了 AI 编程中遇到的问题和他的思路。对我非常有启发。我用 Agent 把它翻译成了中文并人工做了润色，你可以在这里读到：AI 编程的盲点。
概括起来，解决 AI 问题的核心要领还是三点：更准确的提示词、更完整的上下文、缩小问题规模。
相信随着技术的发展，编程范式会发生翻天覆地的变化。如果重构变得如此容易，那么马丁福勒的《重构》是否应该出一套 AI 时代下的新范式。如果文档不再是被人读，而是喂给模型当作上下文，那么文档的形态应该是什么样？是否提供一个向量化的文档接口供大模型调用，将是未来编程框架的新常态？
我对未来充满期待。"><meta property="og:locale" content="zh_CN"><meta property="og:type" content="article"><meta property="article:section" content="posts"><meta property="article:published_time" content="2025-03-23T00:00:01+08:00"><meta property="article:modified_time" content="2025-03-23T00:00:01+08:00"><meta property="article:tag" content="AI"><meta property="og:image" content="https://sund.site/images/share.png"><meta name=twitter:card content="summary_large_image"><meta name=twitter:image content="https://sund.site/images/share.png"><meta name=twitter:title content="与AI协作编程──痛点篇"><meta name=twitter:description content="在与 AI 协作编程中，经常遇到一些大模型无法正确执行的情况。最常见的有：
任务死循环 模型无法修复环境问题 模型执行长任务后半段忘记上下文 一些使用经验 以我自己为例，我经常使用 Cline + Github Copilot 的组合。我很喜欢 Cline 的功能是 Checkpoint restore，它可以在执行错误的位置重新编辑提示词执行。这让我可以在相同的任务中调用不同的模型，观察他们处理问题的能力。
用作规划（Plan）的模型通常用 Deepseek-R1，Gemini 2.0 Flash Thinking，Claude 3.7。这里除了 Claude 3.7 能够比较准确给出计划外，其他模型多少都容易走「歪路」， 比如 Deepseek-R1 喜欢做一些多余的事情，让它翻译中文，它会调用 MCP 的翻译服务而不是自己翻译。
从经济角度考虑，解决简单问题 Gemini 2.0 Flash Thinking 是比较快速、经济的模型。复杂问题直接上 Claude 3.7 可能更容易控制成本。
用作执行任务（Act）的模型里，Deepseek-V3 表现非常不稳定，经常死循环或丢失上下文。Claude 太贵，而 Gemini 2.0 Flash 是相对准确且划算的模型。置于国产的 Qwen 系列模型不完全支持 Function Calling，Cline 也没有适配，所以暂时无法测试。
AI 编程疑难杂症的应对方法 最近读到AI Blindspots这篇文章，作者系统性整理了 AI 编程中遇到的问题和他的思路。对我非常有启发。我用 Agent 把它翻译成了中文并人工做了润色，你可以在这里读到：AI 编程的盲点。
概括起来，解决 AI 问题的核心要领还是三点：更准确的提示词、更完整的上下文、缩小问题规模。
相信随着技术的发展，编程范式会发生翻天覆地的变化。如果重构变得如此容易，那么马丁福勒的《重构》是否应该出一套 AI 时代下的新范式。如果文档不再是被人读，而是喂给模型当作上下文，那么文档的形态应该是什么样？是否提供一个向量化的文档接口供大模型调用，将是未来编程框架的新常态？
我对未来充满期待。"><meta itemprop=name content="与AI协作编程──痛点篇"><meta itemprop=description content="在与 AI 协作编程中，经常遇到一些大模型无法正确执行的情况。最常见的有：
任务死循环 模型无法修复环境问题 模型执行长任务后半段忘记上下文 一些使用经验 以我自己为例，我经常使用 Cline + Github Copilot 的组合。我很喜欢 Cline 的功能是 Checkpoint restore，它可以在执行错误的位置重新编辑提示词执行。这让我可以在相同的任务中调用不同的模型，观察他们处理问题的能力。
用作规划（Plan）的模型通常用 Deepseek-R1，Gemini 2.0 Flash Thinking，Claude 3.7。这里除了 Claude 3.7 能够比较准确给出计划外，其他模型多少都容易走「歪路」， 比如 Deepseek-R1 喜欢做一些多余的事情，让它翻译中文，它会调用 MCP 的翻译服务而不是自己翻译。
从经济角度考虑，解决简单问题 Gemini 2.0 Flash Thinking 是比较快速、经济的模型。复杂问题直接上 Claude 3.7 可能更容易控制成本。
用作执行任务（Act）的模型里，Deepseek-V3 表现非常不稳定，经常死循环或丢失上下文。Claude 太贵，而 Gemini 2.0 Flash 是相对准确且划算的模型。置于国产的 Qwen 系列模型不完全支持 Function Calling，Cline 也没有适配，所以暂时无法测试。
AI 编程疑难杂症的应对方法 最近读到AI Blindspots这篇文章，作者系统性整理了 AI 编程中遇到的问题和他的思路。对我非常有启发。我用 Agent 把它翻译成了中文并人工做了润色，你可以在这里读到：AI 编程的盲点。
概括起来，解决 AI 问题的核心要领还是三点：更准确的提示词、更完整的上下文、缩小问题规模。
相信随着技术的发展，编程范式会发生翻天覆地的变化。如果重构变得如此容易，那么马丁福勒的《重构》是否应该出一套 AI 时代下的新范式。如果文档不再是被人读，而是喂给模型当作上下文，那么文档的形态应该是什么样？是否提供一个向量化的文档接口供大模型调用，将是未来编程框架的新常态？
我对未来充满期待。"><meta itemprop=datePublished content="2025-03-23T00:00:01+08:00"><meta itemprop=dateModified content="2025-03-23T00:00:01+08:00"><meta itemprop=wordCount content="68"><meta itemprop=image content="https://sund.site/images/share.png"><meta itemprop=keywords content="AI"><meta name=referrer content="no-referrer-when-downgrade"><style>:root{--width-max:720px;--font-primary:Verdana, sans-serif;--font-secondary:monospace;--font-size-primary:1em;--font-size-secondary:0.8em;--body-bg-color:#fcfcfc;--bold-text-color:#222;--body-text-color:#444;--link-color:#3273dc;--link-visited-color:#8b6fcb;--table-border-color:#f2f2f2;--table-th-bg-color:#f2f2f2;--img-border-color:#f2f2f2;--code-bg-color:#f2f2f2;--code-text-color:#222;--blockquote-border-color:#666;--blockquote-text-color:#666;--upvoted-color:#FA8072}@media(prefers-color-scheme:dark){:root{--body-bg-color:#1c1c1c;--bold-text-color:#eee;--body-text-color:#ddd;--link-color:#8cc2dd;--link-visited-color:#c3b1ee;--table-border-color:#999;--table-th-bg-color:#999;--img-border-color:#999;--code-bg-color:#555;--code-text-color:#ddd;--blockquote-border-color:#ccc;--blockquote-text-color:#ccc}}body{font-family:var(--font-primary);font-size:var(--font-size-primary);margin:auto;padding:20px;max-width:var(--width-max);text-align:left;background-color:var(--body-bg-color);word-wrap:break-word;overflow-wrap:break-word;line-height:1.5;color:var(--body-text-color)}h1,h2,h3,h4,h5,h6,strong,b{color:var(--bold-text-color)}h1,h2,h3,h4,h5,h6{margin:16px 0}a{color:var(--link-color);cursor:pointer;text-decoration:none}a:hover{text-decoration:underline}.title{text-decoration:none;border:0}.title:hover{text-decoration:none}.title span{font-weight:400}nav a{margin-right:8px}textarea{width:100%;font-size:16px}input{font-size:14px}content{line-height:1.6}table{width:100%;border-collapse:collapse;border:1px solid var(--table-border-color);border-radius:4px}th,td{border:1px solid var(--table-border-color);padding:4px}th{background-color:var(--table-th-bg-color)}hr{border:0;border-top:1px dashed}img{max-width:100%;display:block;margin-left:auto;margin-right:auto;border:1px solid var(--img-border-color);border-radius:4px;content-visibility:auto;loading:lazy}img[src*="#minipic"]{max-width:50%;margin-left:0;margin-right:auto}i{font-style:normal}time{font-family:var(--font-secondary);font-size:15px}code{font-family:var(--font-secondary);background-color:var(--code-bg-color);color:var(--code-text-color);padding:2px;border-radius:4px}pre code{display:block;padding:16px;white-space:pre-wrap;overflow-x:auto}div.highlight pre{border-radius:4px}div.highlight code{background-color:var(--code-bg-color);color:var(--code-text-color)}blockquote{border-left:2px solid var(--blockquote-border-color);color:var(--blockquote-text-color);margin:0;padding-left:16px;font-style:normal}blockquote p{margin:0}footer{padding:25px 0;text-align:center;font-size:var(--font-size-secondary)}ul li:has(input){list-style-type:none;margin-left:-25.5px}ul.blog-posts{list-style-type:none;padding:unset}ul.blog-posts li{display:flex}ul.blog-posts li span{flex:0 0 130px}ul.blog-posts li span.grouped{flex:0 0 80px}ul.blog-posts li a:visited{color:var(--link-visited-color)}div.toc{position:fixed;top:50%;left:calc((100vw + var(--width-max))/2);transform:translateY(-50%);width:calc((90vw - var(--width-max))/2);max-height:80vh;overflow-y:auto;padding:20px 8px;z-index:99;&::-webkit-scrollbar { display:none; } -ms-overflow-style:none;scrollbar-width:none}div.toc ul{list-style-type:none;padding-left:0}div.toc ul li{margin:8px 0}div.toc ul li a{text-decoration:none;color:var(--blockquote-text-color)}div.toc ul li a:hover{color:var(--link-color)}button.upvote-btn{margin:0;padding:0;border:none;background:0 0;cursor:pointer;display:flex;flex-direction:column;align-items:center;color:var(--body-text-color)}button.upvoted{color:var(--upvoted-color)}span.upvote-count{margin-top:-4px;font-size:smaller}@media(max-width:500px){img[src*="#minipic"]{max-width:100%;margin-left:auto;margin-right:auto}div.toc{display:none}}</style></head><body><header><a href=/ class=title><h1>Steve Sun</h1></a><nav><a href=/>Home</a>
<a href=/friends/>Friends</a>
<a href=/posts/>Posts</a></nav></header><main><content><p>在与 AI 协作编程中，经常遇到一些大模型无法正确执行的情况。最常见的有：</p><ul><li>任务死循环</li><li>模型无法修复环境问题</li><li>模型执行长任务后半段忘记上下文</li></ul><h2 id=一些使用经验>一些使用经验</h2><p>以我自己为例，我经常使用 Cline + Github Copilot 的组合。我很喜欢 Cline 的功能是 <code>Checkpoint restore</code>，它可以在执行错误的位置重新编辑提示词执行。这让我可以在相同的任务中调用不同的模型，观察他们处理问题的能力。</p><p>用作规划（Plan）的模型通常用 Deepseek-R1，Gemini 2.0 Flash Thinking，Claude 3.7。这里除了 Claude 3.7 能够比较准确给出计划外，其他模型多少都容易走「歪路」， 比如 Deepseek-R1 喜欢做一些多余的事情，让它翻译中文，它会调用 MCP 的翻译服务而不是自己翻译。</p><p>从经济角度考虑，解决简单问题 Gemini 2.0 Flash Thinking 是比较快速、经济的模型。复杂问题直接上 Claude 3.7 可能更容易控制成本。</p><p>用作执行任务（Act）的模型里，Deepseek-V3 表现非常不稳定，经常死循环或丢失上下文。Claude 太贵，而 Gemini 2.0 Flash 是相对准确且划算的模型。置于国产的 Qwen 系列模型不完全支持 Function Calling，Cline 也没有适配，所以暂时无法测试。</p><h2 id=ai-编程疑难杂症的应对方法>AI 编程疑难杂症的应对方法</h2><p>最近读到<a href=https://ezyang.github.io/ai-blindspots/>AI Blindspots</a>这篇文章，作者系统性整理了 AI 编程中遇到的问题和他的思路。对我非常有启发。我用 Agent 把它翻译成了中文并人工做了润色，你可以在这里读到：<a href=https://sund.notion.site/AI-1be8ce9d275d80649a29e541d310d5c5>AI 编程的盲点</a>。</p><p>概括起来，解决 AI 问题的核心要领还是三点：更准确的提示词、更完整的上下文、缩小问题规模。</p><p>相信随着技术的发展，编程范式会发生翻天覆地的变化。如果重构变得如此容易，那么马丁福勒的《重构》是否应该出一套 AI 时代下的新范式。如果文档不再是被人读，而是喂给模型当作上下文，那么文档的形态应该是什么样？是否提供一个向量化的文档接口供大模型调用，将是未来编程框架的新常态？</p><p>我对未来充满期待。</p></content><p><a href=https://sund.site/tags/ai/>#AI</a></p><div class=toc><nav id=TableOfContents><ul><li><a href=#一些使用经验>一些使用经验</a></li><li><a href=#ai-编程疑难杂症的应对方法>AI 编程疑难杂症的应对方法</a></li></ul></nav></div></main><footer>Subscribe via <a href=/index.xml>RSS</a>.<br>Made with
<a href=https://github.com/rokcso/hugo-bearblog-neo/>Hugo Bear Neo</a>.<br>Copyright © 2013-2025, Steve Sun.
🗺️ <a href=/sitemap.xml>Sitemap</a>.</footer></body></html>