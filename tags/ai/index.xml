<?xml version="1.0" encoding="utf-8" standalone="yes"?><rss version="2.0" xmlns:atom="http://www.w3.org/2005/Atom"><channel><title>AI on Steve Sun</title><link>https://sund.site/tags/ai/</link><description>Recent content in AI on Steve Sun</description><generator>Hugo -- gohugo.io</generator><language>zh-cn</language><copyright>Steve Sun</copyright><lastBuildDate>Sun, 23 Mar 2025 00:00:01 +0800</lastBuildDate><atom:link href="https://sund.site/tags/ai/index.xml" rel="self" type="application/rss+xml"/><item><title>与AI协作编程──痛点篇</title><link>https://sund.site/posts/2025/pairing-with-ai-02/</link><pubDate>Sun, 23 Mar 2025 00:00:01 +0800</pubDate><guid>https://sund.site/posts/2025/pairing-with-ai-02/</guid><description>&lt;p>在与 AI 协作编程中，经常遇到一些大模型无法正确执行的情况。最常见的有：&lt;/p>
&lt;ul>
&lt;li>任务死循环&lt;/li>
&lt;li>模型无法修复环境问题&lt;/li>
&lt;li>模型执行长任务后半段忘记上下文&lt;/li>
&lt;/ul>
&lt;h2 id="一些使用经验">一些使用经验
&lt;/h2>&lt;p>以我自己为例，我经常使用 Cline + Github Copilot 的组合。我很喜欢 Cline 的功能是 &lt;code>Checkpoint restore&lt;/code>，它可以在执行错误的位置重新编辑提示词执行。这让我可以在相同的任务中调用不同的模型，观察他们处理问题的能力。&lt;/p>
&lt;p>用作规划（Plan）的模型通常用 Deepseek-R1，Gemini 2.0 Flash Thinking，Claude 3.7。这里除了 Claude 3.7 能够比较准确给出计划外，其他模型多少都容易走「歪路」， 比如 Deepseek-R1 喜欢做一些多余的事情，让它翻译中文，它会调用 MCP 的翻译服务而不是自己翻译。&lt;/p>
&lt;p>从经济角度考虑，解决简单问题 Gemini 2.0 Flash Thinking 是比较快速、经济的模型。复杂问题直接上 Claude 3.7 可能更容易控制成本。&lt;/p>
&lt;p>用作执行任务（Act）的模型里，Deepseek-V3 表现非常不稳定，经常死循环或丢失上下文。Claude 太贵，而 Gemini 2.0 Flash 是相对准确且划算的模型。置于国产的 Qwen 系列模型不完全支持 Function Calling，Cline 也没有适配，所以暂时无法测试。&lt;/p>
&lt;h2 id="ai-编程疑难杂症的应对方法">AI 编程疑难杂症的应对方法
&lt;/h2>&lt;p>最近读到&lt;a class="link" href="https://ezyang.github.io/ai-blindspots/" target="_blank" rel="noopener"
>AI Blindspots&lt;/a>这篇文章，作者系统性整理了 AI 编程中遇到的问题和他的思路。对我非常有启发。我用 Agent 把它翻译成了中文并人工做了润色，你可以在这里读到：&lt;a class="link" href="https://sund.notion.site/AI-1be8ce9d275d80649a29e541d310d5c5" target="_blank" rel="noopener"
>AI 编程的盲点&lt;/a>。&lt;/p>
&lt;p>概括起来，解决 AI 问题的核心要领还是三点：更准确的提示词、更完整的上下文、缩小问题规模。&lt;/p>
&lt;p>相信随着技术的发展，编程范式会发生翻天覆地的变化。如果重构变得如此容易，那么马丁福勒的《重构》是否应该出一套 AI 时代下的新范式。如果文档不再是被人读，而是喂给模型当作上下文，那么文档的形态应该是什么样？是否提供一个向量化的文档接口供大模型调用，将是未来编程框架的新常态？&lt;/p>
&lt;p>我对未来充满期待。&lt;/p></description></item><item><title>与AI协作编程──测试篇</title><link>https://sund.site/posts/2024/pairing-with-ai-01/</link><pubDate>Wed, 11 Dec 2024 17:02:43 +0800</pubDate><guid>https://sund.site/posts/2024/pairing-with-ai-01/</guid><description>&lt;p>未来的程序开发范式，将是人与 AI 协作编程。这已经是软件行业不争的事实。像 Windsurf，Cusor，Copilot 之类的编程工具一方面提高了开发效率，另一方面也让代码变得更黑盒，更不易阅读和维护。&lt;/p>
&lt;p>我试图浅显地讨论一下哪些软件开发的手段更适合在 AI 时代提高 AI 编写代码的可观测性和维护性。接下来所有以「与 AI 协作编程」为标题的文章都只是抛砖引玉，并未形成系统化方法论。期望任何错误之处，读者不吝赐教。&lt;/p>
&lt;h2 id="使用-ai-编写代码有哪些常见问题">使用 AI 编写代码有哪些常见问题？
&lt;/h2>&lt;p>&lt;strong>可观测性问题：AI 实现的功能不完备，经常要手动修改片段&lt;/strong>&lt;/p>
&lt;p>AI 生成代码最大的问题在于，它经常引起人类不易察觉的隐蔽错误。当人类使用 prompt 修改代码时，由于 AI 行为的不易观测性，即便修复了一个 bug，也可能导致其他回归问题（引起已有逻辑的错误）。&lt;/p>
&lt;p>&lt;strong>上下文问题：缺少全局上下文，碎片代码之间缺少联系&lt;/strong>&lt;/p>
&lt;p>由于 Token 数限制或经济上的考虑，很多编辑器会优化输入的内容，这就容易造成大模型错误地理解局部上下文。没有办法处理跨功能模块的业务逻辑。尤其项目变得庞大后，复杂的模块经常依赖其他模块，调整业务逻辑需要重构若干个代码文件。&lt;/p>
&lt;h2 id="解决思路">解决思路
&lt;/h2>&lt;p>AI 编写代码的核心问题，可以归纳为不可观测性和缺少上下文造成的低维护性。为了解决这两个问题，我们需要先回顾一下传统软件工序如何让代码更易观测和维护。&lt;/p>
&lt;h3 id="人类主导的单元测试">人类主导的单元测试
&lt;/h3>&lt;p>单元测试是代码的说明书。复杂的业务逻辑通常需要阅读大量代码才能看懂。但是熟练的程序员会先看单元测试。好的单元测试会把模块的预期输入、输出完整地写进 Case 里。在 &lt;a class="link" href="https://www.amazon.com/Unit-Testing-Principles-Practices-Patterns/dp/1617296279" target="_blank" rel="noopener"
>Unit Testing Principles, Practices, and Patterns&lt;/a> 里，作者认为好的单元测试应该具备：&lt;/p>
&lt;ul>
&lt;li>&lt;strong>保护回归&lt;/strong>。即测试能够防止出现已经修复的问题在回归测试中复现的情况。&lt;/li>
&lt;li>&lt;strong>抵抗重构&lt;/strong>。即代码重构后，测试能正确识别出重构是否对已有功能造成影响。&lt;/li>
&lt;li>&lt;strong>快速反馈&lt;/strong>。即单元测试容易运行，发现问题能及时定位到错误。&lt;/li>
&lt;li>&lt;strong>易于维护&lt;/strong>。 测试不同于业务代码，它的可维护体现在正确处理依赖关系和共享代码。&lt;/li>
&lt;/ul>
&lt;p>这些原则最终目的，都是保证被测系统按预期行为运行。&lt;/p>
&lt;p>当 AI 和人类合作完成代码时，我个人认为，在编写单元测试这件事上，人类应该主导（80%），AI 辅助（20%），因为单元测试定义了「我期望的行为」。&lt;/p>
&lt;p>当单元测试完善后，又反过来指导 AI 实现的真正的业务代码。这时人类占比下降，AI 占据主导。人类反复运行单元测试，同时将测试结果和 prompt 一起传递给 AI，帮助 AI 修正程序的问题。&lt;/p>
&lt;h3 id="编写对-ai-友好的测试离不开好的模块设计">编写对 AI 友好的测试离不开好的模块设计
&lt;/h3>&lt;p>在编写好的测试时，也要关注正确的拆分模块。一个好的测试通常是给定输入，验证是否输出预期的结果。而模块如果依赖过多外部环境做分支判断，就会造成测试的输出严重依赖外部状态。这会降低模块的可观测性。&lt;/p>
&lt;p>下面两条经验，可以帮助你写出好的代码：&lt;/p>
&lt;ol>
&lt;li>
&lt;p>写测试时要测试行为的结果，而不是步骤。写业务代码时，要 AI 写清步骤。&lt;/p>
&lt;p>单元测试的「单元」可以不是一个类或函数。而是一组完成一个原子业务逻辑的操作。（当然也有不同的流派支持以类为单位测试，但这不是本文的重点）。为了让 AI 生成的业务代码具有抗重构特性，要验证 AI 的行为结果，而不是验证每一个实现步骤。耦合测试代码和实现步骤会导致业务的修改破坏已有的测试，使得「期望的行为」要不断随着「具体的实现」来修改。&lt;/p>
&lt;p>当 AI 开始写业务逻辑后，应该以步骤驱动的方式逐步实现，期间，人类可以针对某一步骤修正 AI 的代码逻辑。但切忌破坏测试的逻辑。&lt;/p>
&lt;/li>
&lt;li>
&lt;p>无状态的代码（函数式）最容易测试&lt;/p>
&lt;p>因为它的输出具有不变性。应该让核心代码尽量无状态，将状态、外部系统依赖放在应用服务层。而把深且不易理解的核心逻辑，放在领域服务层。这里的细节可以参考 DDD（Domain Driven Design）的思想。&lt;/p>
&lt;p>&lt;img src="https://sund.site/images/pairing-with-ai-01/functional_core.png"
loading="lazy"
alt="functional_core.png"
>&lt;/p>
&lt;/li>
&lt;/ol>
&lt;h2 id="小结">小结
&lt;/h2>&lt;p>这篇文章作为一系列人类与 AI 协作编程话题的开头，从测试角度试图缓解 AI 生成代码的可观测性问题。&lt;/p>
&lt;p>在后边的文章里，我希望从架构设计角度，讨论一下如何设计 AI 友好的、易于维护上下文的架构。&lt;/p>
&lt;p>文章内容会随着时间的推移，持续更新，欢迎讨论。&lt;/p></description></item></channel></rss>